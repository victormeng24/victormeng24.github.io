---
title: 记一次工作中的调优经历
date: 2019-08-20 18:25:57
tags:
- 杂谈
---
# Background

BAT某部门，代码有年头了，基本已经到了需要彻底重构的时候，接到这个任务花了大概三周时间重构了一波底层的计算逻辑，直观上的响应时间有了非常大的提升。
￼
简单交代一下背景，业务相关的部分都会隐去。
我们的报表展示平台会展示趋势图数据，当这份数据的时间线很长且每天的维度值很多的时候，目前的服务端响应时间非常慢，特别是当这个展示需求还有多个衍生指标（衍生指标的概念大家搞过数据应该都知道，正常的指标比如现金消费点击，衍生指标就是类似普通指标的日环比&周同比&七日均值&占比等等）的时候，更会慢的令人发指。用缓存当然是最直观也最省事的方案，然而老板们依然非常在意未命中缓存时的响应时间，再加上目前的代码也确实有明显的问题，所以开始重构。

# Process

问题有了，目标也很明确，就是快快快，怎么快呢？当然首先要分析代码，看看到底是慢在了什么地方，哪些地方有较大的优化空间。
我们的服务端处理流程其实可以归纳为三个步骤：
1. 数据库取数据IO到内存
2.  数据加工，核心是根据基础指标计算衍生指标
3.  数据序列化成Json返回给FE展示

我们根据不同的数据量级，对这三个步骤的响应时间进行测试分析，形成如下所示的分析结果：

![优化前的时间占比][image-1]

结果的横坐标以数据行数为单位，默认单行单指标计算5种衍生指标。我理解正常来说，丛数据库fetch数据的时间（在有连接池的情况下）速度基本是没什么太大优化空间的，瓶颈主要在计算和序列化上面。尤其是目前的代码，可以看到序列化占用了相当一大部分时间，十分不合理；此外，计算衍生指标作为在JVM内存计算的步骤，这个时间也有些过长了，主要原因在于目前的计算步骤是串行进行，不能用多线程进行并发。
针对计算问题，目前之所以不能并发的原因是因为我们目前采取的数据结构是基于Google的Table类型，可以理解为是两个HashMap进行的嵌套，自然线程不安全，我对这个数据结构进行了简单的转化使它能够支持并发计算：

![模型变化][image-2]

不知能否表述清楚，如果我们对Table进行并发的加列操作，一定是线程不安全的，而如果我们是对固定列里面线程安全的Object（即ConcurrentHashmap）进行并发操作，这样是没有问题的。更改了数据模型后，我们就可以根据衍生指标的数量来启动对应的线程数进行并发计算。
关于序列化呢，其实有很多的序列化API，比如Protobuf&fasterxml&fastjson等等。其实光看benchmark、测试报告，相差无几，主要还是看使用者自己怎么用。我这边的优化方案只是摒弃掉以前的序列化方法，换成了fasterxml基于Streaming的API，原因很简单，因为它的Introduction里面写着Streaming API有Best Efficiency，然后我就试了一下，真的快了很多：）我觉得多看官网真的收获很大，国内的许多博客一是技术都是比较老的东西，二是讲的内容真的比较浅。
优化过后的响应时间占比看起来就比较合理了，首先序列化占比非常低，其次内存计算的时间随着数据量的膨胀逐渐占比升高。
![优化后的时间占比][image-3]

![][image-4]

benchmark是取的50次的平均值（排除冷启动），可以看到性能提升还是非常明显，此外内存的消耗也被压了很多，这个属于前人的代码问题吧，有比较多无用的拷贝。此外还优化了一波数据库的链接数和代码的可读性，对自己这波重构还是比较满意的。
后续先给自己立几个flag吧，一个是想研究下docker的资源隔离是如何实现的；二是之前看过ConcurrentHashMap的代码，感觉很屌，但感觉还需要写点东西加深下理解。END

[image-1]:	https://myblog-1259548259.cos.ap-beijing.myqcloud.com/be_analysis_before.png "优化前的时间占比"
[image-2]:	https://myblog-1259548259.cos.ap-beijing.myqcloud.com/be_datastructure.png "模型变化"
[image-3]:	https://myblog-1259548259.cos.ap-beijing.myqcloud.com/be_analysis_after.png "优化后的时间占比"
[image-4]:	https://myblog-1259548259.cos.ap-beijing.myqcloud.com/be_benchmark.png "benchmark"