---
title: MIT-6.824-2018-LAB1-MapReduce
date: 2020-06-06 12:30:16
tags:
- Golang
- 分布式系统
---
6.824，MIT的神课之一，转Golang的机会正好作为一次练习。目前Lab1已经刷完，感觉还是很有收获，尤其是踩了些Go的坑，[Github地址][1]。
# Part I
> The Map/Reduce implementation you are given is missing some pieces. Before you can write your first Map/Reduce function pair, you will need to fix the sequential implementation. In particular, the code we give you is missing two crucial pieces: the function that divides up the output of a map task, and the function that gathers all the inputs for a reduce task. These tasks are carried out by the doMap() function in common_map.go, and the doReduce() function in common_reduce.go respectively. The comments in those files should point you in the right direction.
> 
> To help you determine if you have correctly implemented doMap() and doReduce(), we have provided you with a Go test suite that checks the correctness of your implementation. These tests are implemented in the file test_test.go. To run the tests for the sequential implementation that you have now fixed, run:_

PartI是对Map/Reduce function的一个简单实现（不是具体的Mr逻辑，而是对于文件操作的骨架实现），主要是帮助你熟悉Golang的一些对于文件IO的语法，整体没有太大难度。
```js
func doMap(
	jobName string, // the name of the MapReduce job
	mapTask int,    // which map task this is
	inFile string,
	nReduce int, // the number of reduce task that will be run ("R" in the paper)
	mapF func(filename string, contents string) []KeyValue,
) {
	//
	// doMap manages one map task: it should read one of the input files
	// (inFile), call the user-defined map function (mapF) for that file's
	// contents, and partition mapF's output into nReduce intermediate files.
	//
	// There is one intermediate file per reduce task. The file name
	// includes both the map task number and the reduce task number. Use
	// the filename generated by reduceName(jobName, mapTask, r)
	// as the intermediate file for reduce task r. Call ihash() (see
	// below) on each key, mod nReduce, to pick r for a key/value pair.
	//
	// mapF() is the map function provided by the application. The first
	// argument should be the input file name, though the map function
	// typically ignores it. The second argument should be the entire
	// input file contents. mapF() returns a slice containing the
	// key/value pairs for reduce; see common.go for the definition of
	// KeyValue.
	//
	// Look at Go's ioutil and os packages for functions to read
	// and write files.
	//
	// Coming up with a scheme for how to format the key/value pairs on
	// disk can be tricky, especially when taking into account that both
	// keys and values could contain newlines, quotes, and any other
	// character you can think of.
	//
	// One format often used for serializing data to a byte stream that the
	// other end can correctly reconstruct is JSON. You are not required to
	// use JSON, but as the output of the reduce tasks *must* be JSON,
	// familiarizing yourself with it here may prove useful. You can write
	// out a data structure as a JSON string to a file using the commented
	// code below. The corresponding decoding functions can be found in
	// common_reduce.go.
	//
	//   enc := json.NewEncoder(file)
	//   for _, kv := ... {
	//     err := enc.Encode(&kv)
	//
	// Remember to close the file after you have written all the values!
	//
	// Your code here (Part I).
	//
	content, err := ioutil.ReadFile(inFile)
	if err != nil {
		panic(err)
	}
	keyValue := mapF(inFile, string(content))
	mapper := make([][]KeyValue, nReduce)
	// determine to which reducer
	for _, v := range keyValue {
		idx := ihash(v.Key) % nReduce
		mapper[idx] = append(mapper[idx], v)
	}
	// write to target File
	for index, v := range mapper {
		targetFileName := reduceName(jobName, mapTask, index)
		fileNameToChoose, err := os.Create(targetFileName)
		if err != nil {
			panic(err)
		}
		ret, err := json.Marshal(v)
		if err != nil {
			panic(err)
		}
		fileNameToChoose.Write(ret)
		fileNameToChoose.Close()
	}
}
func doReduce(
	jobName string, // the name of the whole MapReduce job
	reduceTask int, // which reduce task this is
	outFile string, // write the output here
	nMap int,       // the number of map tasks that were run ("M" in the paper)
	reduceF func(key string, values []string) string,
) {
	//
	// doReduce manages one reduce task: it should read the intermediate
	// files for the task, sort the intermediate key/value pairs by key,
	// call the user-defined reduce function (reduceF) for each key, and
	// write reduceF's output to disk.
	//
	// You'll need to read one intermediate file from each map task;
	// reduceName(jobName, m, reduceTask) yields the file
	// name from map task m.
	//
	// Your doMap() encoded the key/value pairs in the intermediate
	// files, so you will need to decode them. If you used JSON, you can
	// read and decode by creating a decoder and repeatedly calling
	// .Decode(&kv) on it until it returns an error.
	//
	// You may find the first example in the golang sort package
	// documentation useful.
	//
	// reduceF() is the application's reduce function. You should
	// call it once per distinct key, with a slice of all the values
	// for that key. reduceF() returns the reduced value for that key.
	//
	// You should write the reduce output as JSON encoded KeyValue
	// objects to the file named outFile. We require you to use JSON
	// because that is what the merger than combines the output
	// from all the reduce tasks expects. There is nothing special about
	// JSON -- it is just the marshalling format we chose to use. Your
	// output code will look something like this:
	//
	// enc := json.NewEncoder(file)
	// for key := ... {
	// 	enc.Encode(KeyValue{key, reduceF(...)})
	// }
	// file.Close()
	//
	// Your code here (Part I).
	//
	keyV := make(map[string][]string)
	for i := 0; i < nMap; i++ {
		mapFileName := reduceName(jobName, i, reduceTask)
		input, err := ioutil.ReadFile(mapFileName)
		if err != nil {
			panic(err)
		}
		var tmpKeyv []KeyValue
		json.Unmarshal(input, &tmpKeyv)
		for _, v := range tmpKeyv {
			keyV[v.Key] = append(keyV[v.Key], v.Value)
		}
	}

	outputFile, err := os.Create(outFile)
	if err != nil {
		panic(err)
	}

	enc := json.NewEncoder(outputFile)
	for key, value := range keyV {
		if err := enc.Encode(KeyValue{key, reduceF(key, value)}); err != nil {
			panic(err)
		}
	}
	if err := outputFile.Close(); err != nil {
		panic(err)
	}
}
```
# Part II
> Now you will implement word count — a simple Map/Reduce example. Look in main/wc.go; you'll find empty mapF() and reduceF() functions. Your job is to insert code so that wc.go reports the number of occurrences of each word in its input. A word is any contiguous sequence of letters, as determined by unicode.IsLetter.
> 
实现一个WordCount的mr function，其实也很简单，主要是熟悉一下Golang的api-\>如何将一篇文章拆分成string的slice？strings. FieldsFunc(s string, f func(rune) bool)。
```js
func mapF(filename string, contents string) []mapreduce.KeyValue {
	// Your code here (Part II).
	strArr := strings.FieldsFunc(contents, func(r rune) bool {
		return !unicode.IsLetter(r)
	})
	ret := make([]mapreduce.KeyValue, 0, len(strArr))
	for _, str := range strArr {
		kv := mapreduce.KeyValue{Key: str, Value: "1"}
		ret = append(ret, kv)
	}
	return ret
}

//
// The reduce function is called once for each key generated by the
// map tasks, with a list of all the values created for that key by
// any map task.
//
func reduceF(key string, values []string) string {
	// Your code here (Part II).
	return fmt.Sprintf("%d", len(values))
}
```
# Part III&IV
> Your current implementation runs the map and reduce tasks one at a time. One of Map/Reduce's biggest selling points is that it can automatically parallelize ordinary sequential code without any extra work by the developer. In this part of the lab, you will complete a version of MapReduce that splits the work over a set of worker threads that run in parallel on multiple cores. While not distributed across multiple machines as in real Map/Reduce deployments, your implementation will use RPC to simulate distributed computation.
> In this part you will make the master handle failed workers. MapReduce makes this relatively easy because workers don't have persistent state. If a worker fails while handling an RPC from the master, the master's call() will eventually return false due to a timeout. In that situation, the master should re-assign the task given to the failed worker to another worker.

> An RPC failure doesn't necessarily mean that the worker didn't execute the task; the worker may have executed it but the reply was lost, or the worker may still be executing but the master's RPC timed out. Thus, it may happen that two workers receive the same task, compute it, and generate output. Two invocations of a map or reduce function are required to generate the same output for a given input (i.e. the map and reduce functions are "functional"), so there won't be inconsistencies if subsequent processing sometimes reads one output and sometimes the other. In addition, the MapReduce framework ensures that map and reduce function output appears atomically: the output file will either not exist, or will contain the entire output of a single execution of the map or reduce function (the lab code doesn't actually implement this, but instead only fails workers at the end of a task, so there aren't concurrent executions of a task).
> 
Part III和Part IV，如果你对Goroutine和Channel不是很了解的话，实现起来就有难度了。PartIII是实现一个Mr的分布式调度器schedule()，RPC通信的框架和Worker的DoTask课程都为我们实现了，我们需要考虑的就是Master节点如何调度Worker完成具体的作业，以及（PartIV）当Worker Failure了之后如何实现高可用（将分配给该Worker的任务重新分配给另一个Worker）。实现要求我们只改动schedule.go中的schedule() function：
```js
func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) {
	var ntasks int
	var n_other int // number of inputs (for reduce) or outputs (for map)
	switch phase {
	case mapPhase:
		ntasks = len(mapFiles)
		n_other = nReduce
	case reducePhase:
		ntasks = nReduce
		n_other = len(mapFiles)
	}

	fmt.Printf("Schedule: %v %v tasks (%d I/Os)\n", ntasks, phase, n_other)
	// All ntasks tasks have to be scheduled on workers. Once all tasks
	// have completed successfully, schedule() should return.
	//
	// Your code here (Part III, Part IV).
	//
	var wg sync.WaitGroup
	for i := 0; i < ntasks; i++ {
		wg.Add(1)
		args := &DoTaskArgs{
			JobName:       jobName,
			File:          mapFiles[i],
			Phase:         phase,
			TaskNumber:    i,
			NumOtherPhase: n_other}
		go func(ch chan string, args *DoTaskArgs) {
			worker := <-registerChan
			for !call(worker, "Worker.DoTask", args, nil) {
				worker = <-registerChan
			}
			// start a goroutine to avoid block
			go func() {
				registerChan <- worker
			}()
			wg.Done()
		}(registerChan, args)
	}
	wg.Wait()
	fmt.Printf("Schedule: %v done\n", phase)
}
```
核心就是通过一个Channel对可用的Worker进行控制，调度任务时首先获取Worker，任务执行完毕后释放Worker。当Worker任务执行失败时重新选择一个可用的Worker。
需要注意的是任务执行完毕后为什么要启动另一个goroutine来释放worker呢？因为registerChan \<- worker是一个阻塞操作，当channel中有worker的时候会阻塞在这里导致主函数无法继续执行。
# Part V
> For this optional no-credit exercise, you will build Map and Reduce functions for generating an inverted index.
> 
> Inverted indices are widely used in computer science, and are particularly useful in document searching. Broadly speaking, an inverted index is a map from interesting facts about the underlying data, to the original location of that data. For example, in the context of search, it might be a map from keywords to documents that contain those words.
> 
> We have created a second binary in main/ii.go that is very similar to the wc.go you built earlier. You should modify mapF and reduceF in main/ii.go so that they together produce an inverted index. Running ii.go should output a list of tuples, one per line, in the following format:
> 
可选题，写的Challenge其实我觉得比3、4简单很多。。实现一个倒排索引，统计单词出现在几篇文章中以及这些文章的list，和PartII的逻辑基本没什么区别，加一个map实现去重即可。
```js
// The mapping function is called once for each piece of the input.
// In this framework, the key is the name of the file that is being processed,
// and the value is the file's contents. The return value should be a slice of
// key/value pairs, each represented by a mapreduce.KeyValue.
func mapF(document string, value string) (res []mapreduce.KeyValue) {
	// Your code here (Part V).
	var (
		slice []string
		mp    map[string]string
		ret   []mapreduce.KeyValue
	)
	slice = strings.FieldsFunc(value, func(r rune) bool {
		return !unicode.IsLetter(r)
	})
	mp = make(map[string]string)
	for _, str := range slice {
		if _, ok := mp[str]; !ok {
			mp[str] = document
		}
	}
	ret = make([]mapreduce.KeyValue, 0, len(mp))
	for k, v := range mp {
		ret  = append(ret, mapreduce.KeyValue{Key: k, Value: v})
	}
	return ret
}

// The reduce function is called once for each key generated by Map, with a
// list of that key's string value (merged across all inputs). The return value
// should be a single output value for that key.
func reduceF(key string, values []string) string {
	// Your code here (Part V).
	sort.Strings(values)
	return fmt.Sprintf("%v %v", len(values), strings.Join(values, ","))
}
```

[1]:	https://github.com/victormeng24/6.824 "Github地址"